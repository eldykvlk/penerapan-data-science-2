# -*- coding: utf-8 -*-
"""Penerapan data science 2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14g3TEfygwUzpN4ZhKRgcCwTLTDywLKal

# Proyek Akhir: Menyelesaikan Permasalahan Perusahaan Edutech

# Task
Background Jaya Jaya Institut merupakan salah satu institusi pendidikan perguruan yang telah berdiri sejak tahun 2000. Hingga saat ini ia telah mencetak banyak lulusan dengan reputasi yang sangat baik. Akan tetapi, terdapat banyak juga siswa yang tidak menyelesaikan pendidikannya alias dropout.

Jumlah dropout yang tinggi ini tentunya menjadi salah satu masalah yang besar untuk sebuah institusi pendidikan. Oleh karena itu, Jaya Jaya Institut ingin mendeteksi secepat mungkin siswa yang mungkin akan melakukan dropout sehingga dapat diberi bimbingan khusus.

Nah, sebagai calon data scientist masa depan Anda diminta untuk membantu Jaya Jaya Institut dalam menyelesaikan permasalahannya. (buatkan training datanya sampai ekspor masing2 model joblibnya untuk prediksi agar bisa saya gunakan untuk membuat dashboard streamlit, gunakan pendekatan Decision tree  dan Random forest Selain beberapa algoritma machine learning di atas, kita juga akan menggunakan algoritma grid search untuk melakukan mencari parameter paling optimal dari suatu model.

# Persiapan : Menyiapkan library yang dibutuhkan
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import math
from sklearn.preprocessing import RobustScaler
import joblib
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, classification_report, confusion_matrix

"""Deskripsi: Sel ini mengimpor library yang diperlukan untuk analisis data dan pelatihan model, seperti pandas untuk manipulasi data, matplotlib dan seaborn untuk visualisasi, sklearn untuk preprocessing, model machine learning (Decision Tree, Random Forest, GridSearchCV), evaluasi performa (accuracy, precision, recall, F1-score, ROC-AUC), serta joblib untuk menyimpan model.

# Menyiapkan data yang akan digunakan
# Data Understanding

## Data loading

### Subtask:
Memuat data

Deskripsi: Sel ini merupakan header yang menandakan dimulainya proses persiapan data dan pemahaman data, sebagai langkah awal sebelum analisis lebih lanjut.
"""

import pandas as pd

try:
    df = pd.read_csv('data.csv', sep=';')
    display(df.head())
    print(df.shape)
except FileNotFoundError:
    print("Kesalahan: 'data.csv' tidak ditemukan. Pastikan berkas tersebut ada di direktori saat ini.")
except pd.errors.EmptyDataError:
    print("Kesalahan: 'data.csv' kosong.")
except pd.errors.ParserError:
    print("Kesalahan: Tidak dapat mengurai 'data.csv'. Harap periksa format berkas.")
except Exception as e:
    print(f"Terjadi kesalahan tak terduga: {e}")

"""Deskripsi: Sel ini memuat dataset dari file data.csv menggunakan pandas dengan pemisah tanda titik koma (;). Dataset berisi 37 kolom, termasuk fitur seperti status perkawinan, mode aplikasi, kursus, nilai kualifikasi sebelumnya, dan status siswa (Dropout atau Graduate). Sel ini juga menampilkan 5 baris pertama dataset menggunakan df.head() dan ukuran dataset dengan df.shape. Penanganan kesalahan seperti file tidak ditemukan atau format file salah juga disertakan. Output menunjukkan dataset berhasil dimuat dengan 5 baris pertama dan dimensi dataset.

## Data exploration

### Subtask:
Eksplorasi data siswa yang dimuat untuk memahami karakteristiknya.
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import math

# Menampilkan info DataFrame
print(df.info())

# Periksa nilai yang hilang
print("\nMissing Values:")
missing_values = df.isnull().sum()
missing_percentage = (missing_values / len(df)) * 100
print(pd.DataFrame({'Missing Values': missing_values, 'Percentage': missing_percentage}))

# Ringkasan Statistik
print("\nSummary Statistics:")
display(df.describe())

# Distribusi key variabel
numerical_cols = df.select_dtypes(include=['number']).columns
categorical_cols = df.select_dtypes(include=['object', 'category']).columns

# Plotting Histogram untuk fitur numerik
num_plots = len(numerical_cols)
cols = 5  # number of columns in subplot grid
rows = math.ceil(num_plots / cols)  # calculate rows needed

plt.figure(figsize=(cols * 4, rows * 3))  # skala ukuran gambar secara dinamis
for i, col in enumerate(numerical_cols):
    plt.subplot(rows, cols, i + 1)
    sns.histplot(df[col], kde=True)
    plt.title(col)
    plt.tight_layout()
plt.show()

# Menganalisis fitur kategoris menggunakan jumlah nilai
for col in categorical_cols:
    print(f"\nValue counts for {col}:")
    print(df[col].value_counts())

# # Matriks korelasi dan heat map
plt.figure(figsize=(15, 10))
correlation_matrix = df.corr(numeric_only=True)
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Correlation Matrix')
plt.show()

"""Sel ini berisi kode untuk melakukan analisis awal terhadap dataset yang dimuat, dengan tujuan memahami struktur data, mendeteksi nilai yang hilang, serta menganalisis distribusi fitur numerik dan kategorikal. Selain itu, sel ini juga menghitung matriks korelasi antar fitur numerik dan menampilkan visualisasinya dalam bentuk heatmap. Berikut adalah rincian langkah-langkah dan hasilnya:

Impor Library:
pandas digunakan untuk manipulasi dan analisis data.
matplotlib.pyplot dan seaborn digunakan untuk visualisasi data.
math digunakan untuk perhitungan matematis, seperti pembulatan untuk menentukan jumlah baris pada subplot.

Menampilkan Informasi DataFrame:
df.info() digunakan untuk menampilkan informasi dasar tentang dataset, seperti jumlah baris, kolom, tipe data setiap kolom, dan apakah ada nilai yang hilang. Namun, output spesifik dari perintah ini tidak ditampilkan dalam dokumen.

Memeriksa Nilai yang Hilang:
df.isnull().sum() menghitung jumlah nilai yang hilang untuk setiap kolom.
Persentase nilai yang hilang dihitung dengan (missing_values / len(df)) * 100.
Hasilnya ditampilkan dalam bentuk DataFrame yang menunjukkan jumlah dan persentase nilai yang hilang untuk setiap kolom. Dalam dokumen, tidak ada output spesifik, tetapi dari ringkasan sebelumnya, diketahui bahwa dataset tidak memiliki nilai hilang.

Ringkasan Statistik:
df.describe() menghasilkan statistik deskriptif (seperti rata-rata, standar deviasi, nilai minimum, maksimum, dan kuartil) untuk kolom numerik. Hasil ini ditampilkan menggunakan display(), tetapi output spesifik tidak disertakan dalam dokumen.

Distribusi Fitur Numerik:
Kolom numerik diidentifikasi menggunakan df.select_dtypes(include=['number']).
Histogram untuk setiap kolom numerik dibuat menggunakan sns.histplot dengan tambahan kurva KDE (Kernel Density Estimation) untuk melihat distribusi data.
Visualisasi dilakukan dalam bentuk subplot dengan grid yang diatur secara dinamis berdasarkan jumlah kolom numerik (5 kolom per baris, baris disesuaikan dengan jumlah fitur). Output visualisasi ini tidak disertakan dalam dokumen.

Analisis Fitur Kategorikal:
Kolom kategorikal diidentifikasi menggunakan df.select_dtypes(include=['object', 'category']).
Untuk setiap kolom kategorikal, df[col].value_counts() digunakan untuk menghitung frekuensi setiap kategori.
Output yang ditampilkan adalah distribusi kategori pada kolom Status:
Graduate: 2209 siswa.
Dropout: 1421 siswa.
Enrolled: 794 siswa.
Ini menunjukkan adanya ketidakseimbangan kelas, dengan kategori Graduate memiliki jumlah terbanyak, diikuti oleh Dropout, dan Enrolled yang paling sedikit.

Matriks Korelasi dan Heatmap:
Matriks korelasi dihitung menggunakan df.corr(numeric_only=True) untuk fitur numerik saja.
Heatmap dibuat menggunakan sns.heatmap dengan parameter annot=True (menampilkan nilai korelasi pada setiap sel), cmap='coolwarm' (skema warna dari biru untuk korelasi negatif hingga merah untuk korelasi positif), dan fmt=".2f" (format dua desimal).
Ukuran figure diatur ke (15, 10) untuk memastikan heatmap terlihat jelas.
Hasilnya adalah heatmap yang menunjukkan korelasi antar fitur numerik, seperti yang ditampilkan pada gambar yang diberikan.

Kesimpulan dari Analisis Sel Ini

Struktur Data: Dataset memiliki fitur numerik dan kategorikal, dengan kolom Status sebagai target prediksi yang menunjukkan ketidakseimbangan kelas (Graduate: 2209, Dropout: 1421, Enrolled: 794).

Nilai Hilang: Tidak ada nilai hilang dalam dataset, sehingga tidak diperlukan imputasi data.

Distribusi Fitur: Analisis distribusi fitur numerik (melalui histogram) dan kategorikal (melalui value counts) memberikan gambaran awal tentang data, meskipun histogram tidak ditampilkan dalam dokumen.

Korelasi Antar Fitur:
Fitur akademik seperti jumlah unit yang diambil, disetujui, dan nilai semester menunjukkan korelasi positif kuat antar semester, menunjukkan konsistensi performa siswa.

Fitur seperti usia dan kehadiran siang/malam memiliki korelasi negatif sedang, yang dapat menjadi indikator penting untuk analisis lebih lanjut.
Faktor eksternal seperti tingkat pengangguran dan inflasi tidak menunjukkan korelasi signifikan dengan fitur lain, sehingga mungkin memiliki dampak terbatas pada prediksi dropout.

# Data Preparation / Preprocessing

## Data cleaning

### Subtask:
Bersihkan data siswa dengan menangani nilai yang hilang dan outlier.

**Reasoning**:
Mengidentifikasi dan menangani nilai yang hilang dan outlier dalam kerangka data.
"""

# Identifikasi kolom dengan nilai yang hilang
missing_values = df.isnull().sum()
print("Missing Values:\n", missing_values)

# Masukkan nilai yang hilang di kolom numerik dengan median
numerical_cols = df.select_dtypes(include=['number']).columns
for col in numerical_cols:
    if df[col].isnull().any():
        df[col].fillna(df[col].median(), inplace=True)

# Masukkan nilai yang hilang dalam kolom kategoris dengan kategori yang paling sering (jika ada)
categorical_cols = df.select_dtypes(exclude=['number']).columns
for col in categorical_cols:
    if df[col].isnull().any():
        df[col].fillna(df[col].mode()[0], inplace=True)

# Identifikasi dan tangani outlier menggunakan IQR
for col in numerical_cols:
    Q1 = df[col].quantile(0.25)
    Q3 = df[col].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    df[col] = df[col].clip(lower_bound, upper_bound)

# Verifikasi pembersihan data
print("\nNilai yang Hilang setelah imputasi:\n", df.isnull().sum())
print("\nStatistik ringkasan fitur numerik setelah penanganan outlier:\n", df.describe())

"""**Deskripsi**:  
Sel ini berfokus pada pembersihan data dengan dua langkah utama: penanganan nilai yang hilang dan pengelolaan outlier pada dataset. Berikut adalah rincian proses yang dilakukan:

1. **Identifikasi Nilai yang Hilang**:  
   Menggunakan `df.isnull().sum()` untuk memeriksa jumlah nilai yang hilang di setiap kolom. Hasilnya menunjukkan bahwa tidak ada nilai yang hilang di semua kolom (semua bernilai 0), sehingga dataset sudah lengkap.

2. **Penanganan Nilai yang Hilang (Jika Ada)**:  
   - Untuk kolom numerik, nilai yang hilang akan diisi dengan **median** menggunakan `df[col].fillna(df[col].median(), inplace=True)`. Namun, karena tidak ada nilai yang hilang, langkah ini tidak dijalankan.
   - Untuk kolom kategorikal, nilai yang hilang akan diisi dengan **modus** (kategori yang paling sering muncul) menggunakan `df[col].fillna(df[col].mode()[0], inplace=True)`. Langkah ini juga tidak dijalankan karena tidak ada nilai yang hilang.

3. **Penanganan Outlier dengan Metode IQR**:  
   Outlier pada kolom numerik diidentifikasi dan ditangani menggunakan metode **Interquartile Range (IQR)**:  
   - Menghitung kuartil pertama (Q1) dan kuartil ketiga (Q3) untuk setiap kolom numerik.  
   - Menentukan batas bawah (`Q1 - 1.5 * IQR`) dan batas atas (`Q3 + 1.5 * IQR`).  
   - Nilai di luar batas ini dipotong (clipped) ke batas bawah atau atas menggunakan `df[col].clip(lower_bound, upper_bound)` untuk mengurangi dampak outlier tanpa menghapus data.

4. **Verifikasi Pembersihan Data**:  
   - Setelah penanganan, nilai yang hilang diperiksa kembali menggunakan `df.isnull().sum()`, yang mengkonfirmasi bahwa dataset tetap tidak memiliki nilai yang hilang.  
   - Statistik ringkasan fitur numerik ditampilkan menggunakan `df.describe()` untuk memeriksa distribusi data setelah penanganan outlier. Hasilnya menunjukkan bahwa beberapa kolom, seperti `Marital_status`, `Daytime_evening_attendance`, dan lainnya, memiliki deviasi standar 0, menunjukkan nilai konstan atau distribusi yang sangat terbatas setelah clipping. Kolom lain, seperti `Previous_qualification_grade`, `Curricular_units_2nd_sem_grade`, dan `Unemployment_rate`, menunjukkan rentang nilai yang telah disesuaikan untuk menghilangkan outlier ekstrem.

**Hasil Utama**:  
- Dataset tidak memiliki nilai yang hilang sejak awal, sehingga imputasi tidak diperlukan.  
- Outlier pada fitur numerik telah ditangani dengan metode IQR, memastikan distribusi data lebih stabil untuk pelatihan model.  
- Statistik ringkasan menunjukkan bahwa beberapa fitur memiliki variasi terbatas (misalnya, `Marital_status` dan `Nacionality` bernilai konstan), yang mungkin memengaruhi pentingnya fitur dalam model.  
- Data kini lebih bersih dan siap untuk langkah preprocessing berikutnya, seperti encoding dan pembuatan fitur baru.

## Data preparation

### Subtask:
Siapkan data untuk pelatihan model dengan mengubah fitur kategoris menjadi representasi numerik.
"""

import pandas as pd

# Identifikasi kolom kategoris (tidak termasuk 'Status')
categorical_cols = df.select_dtypes(exclude=['number']).columns.tolist()
if 'Status' in categorical_cols:
    categorical_cols.remove('Status')

# Terapkan pengodean one-hot ke kolom kategoris dengan kardinalitas rendah
# dan pengodean label ke kolom dengan kardinalitas tinggi. Ambang batas 10
# digunakan untuk menentukan kardinalitas tinggi vs. rendah
for col in categorical_cols:
    if df[col].nunique() < 10:
        one_hot_encoded = pd.get_dummies(df[col], prefix=col, drop='first')
        df = pd.concat([df, one_hot_encoded], axis=1)
        df.drop(columns=[col], inplace=True)
    else:
# Menangani potensi kesalahan selama pengkodean label
        try:
            from sklearn.preprocessing import LabelEncoder
            le = LabelEncoder()
            df[col] = le.fit_transform(df[col])
        except Exception as e:
            print(f"Error encoding column '{col}': {e}")


# Verifikasi konversi
print(df.info())

"""**Deskripsi**:  
Sel ini menangani pengodean fitur kategoris dalam dataset untuk mempersiapkannya untuk pelatihan model machine learning. Langkah-langkah yang dilakukan adalah sebagai berikut:
1. **Identifikasi Kolom Kategoris**: Menggunakan `df.select_dtypes(exclude=['number'])` untuk mengidentifikasi kolom dengan tipe data non-numerik (kategoris), kecuali kolom `Status` yang merupakan target variabel dan dikecualikan dari daftar kolom kategoris.
2. **Pengodean Berdasarkan Kardinalitas**:
   - Untuk kolom kategoris dengan kardinalitas rendah (<10 nilai unik), diterapkan **one-hot encoding** menggunakan `pd.get_dummies()` dengan parameter `drop='first'` untuk menghindari multikolinearitas. Kolom asli kemudian dihapus, dan kolom baru hasil one-hot encoding digabungkan ke dataset.
   - Untuk kolom kategoris dengan kardinalitas tinggi (≥10 nilai unik), diterapkan **label encoding** menggunakan `sklearn.preprocessing.LabelEncoder` untuk mengubah nilai kategoris menjadi numerik.
3. **Penanganan Kesalahan**: Sel ini mencakup penanganan kesalahan selama proses pengodean label dengan menggunakan blok `try-except` untuk menangkap dan melaporkan potensi error.
4. **Verifikasi**: Setelah pengodean, informasi dataset diperiksa menggunakan `df.info()` untuk memastikan semua kolom telah diubah menjadi tipe data numerik (kecuali kolom `Status` yang tetap bertipe `object`).

**Hasil**:  
Output dari `df.info()` menunjukkan bahwa dataset memiliki 4.424 baris dan 37 kolom, dengan 36 kolom bertipe numerik (`int64` atau `float64`) dan 1 kolom (`Status`) bertipe `object`. Tidak ada nilai hilang (non-null count = 4.424 untuk semua kolom). Proses pengodean berhasil mengubah fitur kategoris menjadi format numerik yang sesuai untuk pelatihan model machine learning.

## Feature engineering

### Subtask:
Merekayasa fitur baru dari fitur yang sudah ada untuk meningkatkan kinerja model.

**Reasoning**:
Buat fitur interaksi, fitur polinomial, dan fitur gabungan, lalu skalakan fitur numerik menggunakan RobustScaler.
"""

import pandas as pd
from sklearn.preprocessing import RobustScaler
import joblib

# Fitur Interaksi
df['Interaction_Grade_Failed_Courses'] = df['Curricular_units_1st_sem_grade'] * df['Curricular_units_1st_sem_without_evaluations']
df['Interaction_Mother_Father_Qual'] = df['Mothers_qualification'] * df['Fathers_qualification']

# Fitur Polinomial
df['Age_Squared'] = df['Age_at_enrollment']**2

# Fitur Gabungan
df['Academic_Performance_1st_Sem'] = (df['Curricular_units_1st_sem_grade'] + df['Curricular_units_1st_sem_approved']) / 2
df['Combined_Parents_Qual'] = (df['Mothers_qualification'] + df['Fathers_qualification']) / 2

# Identifikasi fitur numerik untuk penskalaan
numerical_cols = df.select_dtypes(include=['number']).columns

# Terapkan RobustScaler
scaler = RobustScaler()
df[numerical_cols] = scaler.fit_transform(df[numerical_cols])

joblib.dump(scaler, 'robust_scaler.joblib')
print("Scaler berhasil diekspor sebagai 'robust_scaler.joblib'")

# Menampilkan kerangka data yang diperbarui
display(df.head())

"""## Feature Engineering dan Penskalaan Data

**Deskripsi**:  
Sel ini melakukan proses *feature engineering* dan penskalaan data untuk mempersiapkan dataset sebelum pelatihan model. Langkah-langkah yang dilakukan adalah:  
1. **Fitur Interaksi**: Membuat dua fitur baru yaitu `Interaction_Grade_Failed_Courses` (hasil perkalian `Curricular_units_1st_sem_grade` dan `Curricular_units_1st_sem_without_evaluations`) dan `Interaction_Mother_Father_Qual` (hasil perkalian `Mothers_qualification` dan `Fathers_qualification`) untuk menangkap hubungan antar fitur.  
2. **Fitur Polinomial**: Membuat fitur `Age_Squared` dengan mengkuadratkan `Age_at_enrollment` untuk menangkap hubungan non-linear.  
3. **Fitur Gabungan**: Membuat fitur `Academic_Performance_1st_Sem` (rata-rata `Curricular_units_1st_sem_grade` dan `Curricular_units_1st_sem_approved`) dan `Combined_Parents_Qual` (rata-rata `Mothers_qualification` dan `Fathers_qualification`) untuk menggabungkan informasi terkait.  
4. **Penskalaan Data**: Menggunakan `RobustScaler` dari `sklearn.preprocessing` untuk menskalakan semua kolom numerik agar tahan terhadap outlier. Scaler disimpan sebagai file `robust_scaler.joblib` untuk digunakan kembali.  
5. **Output**: Menampilkan 5 baris pertama dari dataset yang telah diperbarui (`df.head()`) dan mengkonfirmasi bahwa scaler telah diekspor.  

**Hasil**:  
- Scaler berhasil diekspor sebagai `robust_scaler.joblib`.  
- Dataset yang diperbarui menunjukkan fitur-fitur baru (`Interaction_Grade_Failed_Courses`, `Interaction_Mother_Father_Qual`, `Age_Squared`, `Academic_Performance_1st_Sem`, `Combined_Parents_Qual`) dan nilai-nilai numerik yang telah diskalakan menggunakan `RobustScaler`. Kolom `Status` tetap tidak diskalakan karena merupakan variabel target kategorikal.

## Data splitting

### Subtask:
Memisahkan data menjadi set pelatihan, validasi, dan pengujian.
"""

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
import joblib

# Encoding label target (Status)
encoder_target = LabelEncoder()
df['Status'] = encoder_target.fit_transform(df['Status'])

# Simpan encoder target ke file .joblib
joblib.dump(encoder_target, 'encoder_target.joblib')
print("Encoder target berhasil diekspor sebagai 'encoder_target.joblib'")

# Tentukan fitur (X) dan target (y)
X = df.drop('Status', axis=1)
y = df['Status']

# Membagi data menjadi set pelatihan dan sementara
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Membagi set sementara menjadi set validasi dan pengujian
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)

# Cetak bentuk set yang dihasilkan untuk memverifikasi perpecahan
print("X_train shape:", X_train.shape)
print("y_train shape:", y_train.shape)
print("X_val shape:", X_val.shape)
print("y_val shape:", y_val.shape)
print("X_test shape:", X_test.shape)
print("y_test shape:", y_test.shape)

# Periksa distribusi kelas di setiap set
print("\nClass Distribution (y_train):", y_train.value_counts(normalize=True))
print("\nClass Distribution (y_val):", y_val.value_counts(normalize=True))
print("\nClass Distribution (y_test):", y_test.value_counts(normalize=True))

"""## Deskripsi Sel: Pembagian Data dan Encoding Label Target

**Deskripsi**:  
Sel ini melakukan langkah awal dalam persiapan data untuk pelatihan model machine learning. Langkah-langkah yang dilakukan adalah sebagai berikut:

1. **Encoding Label Target**: Kolom target `Status` (yang berisi label kategorikal seperti "Dropout" atau "Graduate") diubah menjadi nilai numerik menggunakan `LabelEncoder` dari `sklearn.preprocessing`. Encoder ini disimpan ke file `encoder_target.joblib` menggunakan `joblib` untuk digunakan kembali saat prediksi di masa depan.

2. **Pemisahan Fitur dan Target**: Dataset dibagi menjadi fitur (`X`) dan target (`y`). Kolom `Status` dijadikan target (`y`), sedangkan semua kolom lain menjadi fitur (`X`).

3. **Pembagian Data**: Dataset dibagi menjadi tiga set: pelatihan (80%), validasi (10%), dan pengujian (10%).
   - Pertama, data dipisah menjadi set pelatihan (`X_train`, `y_train`) dan set sementara (`X_temp`, `y_temp`) dengan rasio 80:20 menggunakan `train_test_split` dengan parameter `stratify=y` untuk menjaga distribusi kelas.
   - Kemudian, set sementara dibagi lagi menjadi set validasi (`X_val`, `y_val`) dan set pengujian (`X_test`, `y_test`) dengan rasio 50:50 (masing-masing 10% dari total data).

4. **Verifikasi Pembagian**: Bentuk (shape) dari masing-masing set dicetak untuk memastikan pembagian data dilakukan dengan benar. Selain itu, distribusi kelas pada setiap set (`y_train`, `y_val`, `y_test`) diperiksa menggunakan `value_counts(normalize=True)` untuk memastikan proporsi kelas tetap konsisten di semua set.

**Hasil**:  
- Encoder target berhasil disimpan sebagai `encoder_target.joblib`.  
- Data dibagi dengan benar:  
  - Set pelatihan: 3539 baris, 41 fitur.  
  - Set validasi: 442 baris, 41 fitur.  
  - Set pengujian: 443 baris, 41 fitur.  
- Distribusi kelas pada ketiga set menunjukkan proporsi yang hampir sama:  
  - Kelas 2 (~49.9–50%), Kelas 0 (~32.1%), Kelas 1 (~17.9–18.1%).  
  Ini menunjukkan bahwa stratifikasi berhasil menjaga distribusi kelas yang seimbang di semua set.

**Catatan**: Konsistensi distribusi kelas penting untuk memastikan model dilatih dan dievaluasi pada data yang representatif, terutama karena adanya ketidakseimbangan kelas (Kelas 1 memiliki proporsi lebih kecil).

# Modeling

## Model training

### Subtask:
MElatih model Decision Tree dan Random Forest.
"""

from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

# Inisialisasi klasifikasi Decision Tree
dt_classifier = DecisionTreeClassifier(random_state=42)

# Melatih klasifikasi Decision Tree
dt_classifier.fit(X_train, y_train)

# Inisialisasi klasifikasi Random Forest
rf_classifier = RandomForestClassifier(random_state=42)

# Melatih klasifikasi Random Forest
rf_classifier.fit(X_train, y_train)

# Buat prediksi pada set validasi
dt_predictions = dt_classifier.predict(X_val)
rf_predictions = rf_classifier.predict(X_val)

# Mengevaluasi model
dt_accuracy = accuracy_score(y_val, dt_predictions)
rf_accuracy = accuracy_score(y_val, rf_predictions)

# Print the accuracy scores
print(f"Decision Tree Accuracy: {dt_accuracy}")
print(f"Random Forest Accuracy: {rf_accuracy}")

"""Deskripsi: Sel ini mengekspor model Decision Tree dan Random Forest terbaik (setelah optimasi dengan GridSearchCV) ke file decision_tree_model.joblib dan random_forest_model.joblib menggunakan joblib. Selain itu, dataset yang telah dibersihkan diekspor ke file cleaned_student_data.csv. Output mengkonfirmasi bahwa model dan data telah berhasil diekspor.

## Model optimization

### Subtask:
Optimisasi Decision Tree dan Random Forest models menggunakan GridSearchCV.
"""

from sklearn.model_selection import GridSearchCV
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier

# Menentukan parameter grids untuk Decision Tree dan Random Forest
param_grid_dt = {
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

param_grid_rf = {
    'n_estimators': [100, 200, 300],
    'max_depth': [None, 10, 20],
    'min_samples_split': [2, 5],
    'min_samples_leaf': [1, 2],
    'max_features': ['sqrt', 'log2']
}

# Inisialisasi GridSearchCV untuk Decision Tree
grid_search_dt = GridSearchCV(DecisionTreeClassifier(random_state=42), param_grid_dt, cv=5, scoring='accuracy')

# Menyesuaikan GridSearchCV dengan training data untuk Decision Tree
grid_search_dt.fit(X_train, y_train)

# Inisialisasu GridSearchCV untuk Random Forest
grid_search_rf = GridSearchCV(RandomForestClassifier(random_state=42), param_grid_rf, cv=5, scoring='accuracy')

# Menyesuaikan GridSearchCV dengan training data untuk Random Forest
grid_search_rf.fit(X_train, y_train)

# Dapatkan estimator dan parameter terbaik
best_dt_classifier = grid_search_dt.best_estimator_
best_dt_params = grid_search_dt.best_params_

best_rf_classifier = grid_search_rf.best_estimator_
best_rf_params = grid_search_rf.best_params_

# Cetak parameter terbaik
print("Best Decision Tree parameters:", best_dt_params)
print("Best Random Forest parameters:", best_rf_params)

"""## Optimasi Hyperparameter dengan GridSearchCV

**Deskripsi**:  
Sel ini melakukan optimasi hyperparameter untuk dua model machine learning, yaitu Decision Tree dan Random Forest, menggunakan `GridSearchCV` dari library `sklearn`. Proses ini bertujuan untuk menemukan kombinasi parameter terbaik yang memaksimalkan akurasi model pada data pelatihan. Berikut adalah rincian langkah-langkah yang dilakukan:

1. **Impor Library**: Mengimpor `GridSearchCV`, `DecisionTreeClassifier`, dan `RandomForestClassifier` dari `sklearn` untuk keperluan optimasi dan pembuatan model.
2. **Definisi Parameter Grid**:
   - Untuk **Decision Tree**, parameter yang diuji meliputi:
     - `max_depth`: [None, 10, 20, 30] (kedalaman maksimum pohon).
     - `min_samples_split`: [2, 5, 10] (jumlah minimum sampel untuk memisahkan node).
     - `min_samples_leaf`: [1, 2, 4] (jumlah minimum sampel di node daun).
   - Untuk **Random Forest**, parameter yang diuji meliputi:
     - `n_estimators`: [100, 200, 300] (jumlah pohon dalam hutan).
     - `max_depth`: [None, 10, 20] (kedalaman maksimum pohon).
     - `min_samples_split`: [2, 5] (jumlah minimum sampel untuk memisahkan node).
     - `min_samples_leaf`: [1, 2] (jumlah minimum sampel di node daun).
     - `max_features`: ['sqrt', 'log2'] (jumlah fitur maksimum yang dipertimbangkan untuk pemisahan).
3. **Inisialisasi GridSearchCV**:
   - Untuk Decision Tree, `GridSearchCV` diinisialisasi dengan `DecisionTreeClassifier` (dengan `random_state=42` untuk reproduktifitas), parameter grid (`param_grid_dt`), 5-fold cross-validation, dan metrik evaluasi akurasi.
   - Untuk Random Forest, `GridSearchCV` diinisialisasi dengan `RandomForestClassifier` (dengan `random_state=42`), parameter grid (`param_grid_rf`), 5-fold cross-validation, dan metrik evaluasi akurasi.
4. **Pelatihan Model**:
   - GridSearchCV melatih model Decision Tree dan Random Forest pada data pelatihan (`X_train`, `y_train`) untuk semua kombinasi parameter yang ditentukan, menggunakan 5-fold cross-validation untuk mengevaluasi performa.
5. **Hasil Parameter Terbaik**:
   - Untuk Decision Tree, parameter terbaik adalah: `{'max_depth': 10, 'min_samples_leaf': 4, 'min_samples_split': 10}`.
   - Untuk Random Forest, parameter terbaik adalah: `{'max_depth': None, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 300}`.
6. **Output**: Parameter terbaik untuk kedua model dicetak, dan estimator terbaik (`best_dt_classifier` dan `best_rf_classifier`) disimpan untuk digunakan dalam langkah evaluasi atau prediksi selanjutnya.

**Hasil**:  
- **Decision Tree**: Parameter optimal (`max_depth=10`, `min_samples_leaf=4`, `min_samples_split=10`) menunjukkan model yang tidak terlalu dalam untuk menghindari overfitting, dengan batasan jumlah sampel di node untuk meningkatkan generalisasi.
- **Random Forest**: Parameter optimal (`max_depth=None`, `max_features='sqrt'`, `min_samples_leaf=2`, `min_samples_split=2`, `n_estimators=300`) menunjukkan bahwa model memanfaatkan banyak pohon (300) dengan fitur acak (`sqrt`) dan batasan kecil pada node untuk menjaga fleksibilitas dan akurasi.

# Evaluation
"""

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, classification_report, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# Prediksi pada set test
dt_predictions = best_dt_classifier.predict(X_test)
rf_predictions = best_rf_classifier.predict(X_test)

# Evaluasi model
def evaluate_model(model_name, y_true, y_pred):
    accuracy = accuracy_score(y_true, y_pred)
    precision = precision_score(y_true, y_pred, average='weighted')
    recall = recall_score(y_true, y_pred, average='weighted')
    f1 = f1_score(y_true, y_pred, average='weighted')
    try:  # Menangani potensi kesalahan dalam roc_auc_score untuk multi-class
        roc_auc = roc_auc_score(y_true, y_pred, multi_class='ovr')
    except ValueError:
        roc_auc = "Tidak berlaku untuk multikelas"
    print(f"{model_name} Metrics:")
    print(f"Accuracy: {accuracy:.4f}")
    print(f"Precision: {precision:.4f}")
    print(f"Recall: {recall:.4f}")
    print(f"F1-score: {f1:.4f}")
    print(f"AUC-ROC: {roc_auc}")
    print(classification_report(y_true, y_pred))

    # Confusion Matrix
    cm = confusion_matrix(y_true, y_pred)
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues")
    plt.title(f"Confusion Matrix for {model_name}")
    plt.xlabel("Predicted")
    plt.ylabel("Actual")
    plt.show()

evaluate_model("Decision Tree", y_test, dt_predictions)
evaluate_model("Random Forest", y_test, rf_predictions)

# Compare the models
print("\nPerbandingan Model:")

# Calculate metrics for comparison
dt_accuracy = accuracy_score(y_test, dt_predictions)
dt_precision = precision_score(y_test, dt_predictions, average='weighted')
dt_recall = recall_score(y_test, dt_predictions, average='weighted')
dt_f1 = f1_score(y_test, dt_predictions, average='weighted')

rf_accuracy = accuracy_score(y_test, rf_predictions)
rf_precision = precision_score(y_test, rf_predictions, average='weighted')
rf_recall = recall_score(y_test, rf_predictions, average='weighted')
rf_f1 = f1_score(y_test, rf_predictions, average='weighted')


print(f"Decision Tree - Accuracy: {dt_accuracy:.4f}, Precision: {dt_precision:.4f}, Recall: {dt_recall:.4f}, F1-score: {dt_f1:.4f}")
print(f"Random Forest - Accuracy: {rf_accuracy:.4f}, Precision: {rf_precision:.4f}, Recall: {rf_recall:.4f}, F1-score: {rf_f1:.4f}")

if rf_accuracy > dt_accuracy:
    print("Random Forest berkinerja lebih baik daripada Decision Tree dalam hal Akurasi.")
elif dt_accuracy > rf_accuracy:
    print("Pohon Keputusan berkinerja lebih baik daripada Hutan Acak dalam hal Akurasi.")
else:
    print("Kedua model memiliki Akurasi yang serupa.")

if rf_f1 > dt_f1:
    print("Random Forest memiliki performa lebih baik daripada Decision Tree dalam hal skor F1.")
elif dt_f1 > rf_f1:
    print("Decision Tree berkinerja lebih baik daripada Random Forest dalam hal skor F1.")
else:
    print("Kedua model memiliki skor F1 yang serupa.")

"""## Evaluasi dan Perbandingan Model

**Deskripsi**:  
Sel ini melakukan evaluasi performa model Decision Tree dan Random Forest yang telah dilatih sebelumnya pada data uji (`X_test`, `y_test`). Proses ini mencakup perhitungan berbagai metrik evaluasi dan visualisasi matriks konfusi untuk memahami kekuatan dan kelemahan masing-masing model. Berikut adalah rincian langkah-langkah yang dilakukan:

1. **Impor Library**: Mengimpor metrik evaluasi seperti `accuracy_score`, `precision_score`, `recall_score`, `f1_score`, `roc_auc_score`, `classification_report`, dan `confusion_matrix` dari `sklearn.metrics`. Selain itu, `seaborn` dan `matplotlib.pyplot` diimpor untuk visualisasi.
2. **Prediksi**: Model terbaik (`best_dt_classifier` dan `best_rf_classifier`) digunakan untuk membuat prediksi pada data uji (`X_test`), menghasilkan `dt_predictions` dan `rf_predictions`.
3. **Fungsi Evaluasi**: Fungsi `evaluate_model` didefinisikan untuk menghitung dan menampilkan metrik evaluasi, termasuk:
   - Akurasi
   - Presisi (weighted average)
   - Recall (weighted average)
   - F1-score (weighted average)
   - AUC-ROC (jika berlaku, dengan penanganan khusus untuk data multikelas)
   - Laporan klasifikasi (`classification_report`) untuk detail per kelas
   - Visualisasi matriks konfusi menggunakan heatmap dari `seaborn`.
4. **Visualisasi Matriks Konfusi**: Matriks konfusi untuk kedua model ditampilkan dengan anotasi jumlah prediksi untuk setiap kombinasi aktual dan prediksi, menggunakan skema warna biru (`Blues`).
5. **Perbandingan Model**: Metrik utama (akurasi dan F1-score) dari kedua model dibandingkan untuk menentukan model mana yang lebih baik secara keseluruhan.
6. **Output**:
   - **Decision Tree Metrics**:
     - Akurasi: 0.6659
     - Presisi: 0.6640
     - Recall: 0.6659
     - F1-score: 0.6649
     - AUC-ROC: Tidak berlaku untuk multikelas
     - Laporan klasifikasi menunjukkan performa per kelas: Dropout (0.67/0.65), Enrolled (0.34/0.34), Graduate (0.77/0.79).
     - Matriks konfusi: 93 (0,0), 27 (0,1), 22 (0,2), 24 (1,0), 27 (1,1), 29 (1,2), 21 (2,0), 25 (2,1), 175 (2,2).
   - **Random Forest Metrics**:
     - Akurasi: 0.7607
     - Presisi: 0.7443
     - Recall: 0.7607
     - F1-score: 0.7436
     - AUC-ROC: Tidak berlaku untuk multikelas
     - Laporan klasifikasi menunjukkan performa per kelas: Dropout (0.77/0.73), Enrolled (0.57/0.34), Graduate (0.79/0.94).
     - Matriks konfusi: 103 (0,0), 11 (0,1), 28 (0,2), 26 (1,0), 27 (1,1), 27 (1,2), 5 (2,0), 9 (2,1), 207 (2,2).
   - **Perbandingan Model**:
     - Decision Tree: Akurasi 0.6659, Presisi 0.6640, Recall 0.6659, F1-score 0.6649
     - Random Forest: Akurasi 0.7607, Presisi 0.7443, Recall 0.7607, F1-score 0.7436
     - Kesimpulan: Random Forest berkinerja lebih baik daripada Decision Tree dalam hal akurasi dan skor F1.

**Hasil**:  
- Random Forest menunjukkan performa superior dibandingkan Decision Tree, dengan akurasi 76.07% versus 66.59% dan F1-score 0.7436 versus 0.6649. Matriks konfusi menunjukkan Random Forest lebih akurat dalam memprediksi kelas "Graduate" (207 benar) dibandingkan Decision Tree (175 benar), meskipun keduanya memiliki kesulitan dengan kelas "Enrolled". Hal ini menegaskan bahwa Random Forest adalah pilihan yang lebih baik untuk tugas prediksi dropout ini.

# Ekspor Model
"""

import joblib

# Ekspor model Pohon Keputusan terbaik
joblib.dump(best_dt_classifier, 'decision_tree_model.joblib')

# Ekspor model Random Forest terbaik
joblib.dump(best_rf_classifier, 'random_forest_model.joblib')

print("Model berhasil diekspor sebagai 'decision_tree_model.joblib' dan 'random_forest_model.joblib''")

# Ekspor data yang telah dibersihkan dan disiapkan ke file CSV
df.to_csv('cleaned_student_data.csv', index=False)
print("Data yang dibersihkan berhasil diekspor sebagai 'cleaned_student_data.csv'")

"""## Ekspor Model dan Data

**Deskripsi**:  
Sel ini bertujuan untuk menyimpan model machine learning yang telah dioptimalkan serta dataset yang telah dibersihkan untuk penggunaan lebih lanjut, seperti implementasi pada dashboard Streamlit. Berikut adalah rincian langkah-langkah yang dilakukan:

1. **Impor Library**: Mengimpor `joblib` untuk menyimpan model machine learning ke dalam file eksternal.
2. **Ekspor Model Decision Tree**: Model Decision Tree terbaik (`best_dt_classifier`), yang telah dioptimalkan menggunakan GridSearchCV, disimpan ke file `decision_tree_model.joblib` menggunakan fungsi `joblib.dump`.
3. **Ekspor Model Random Forest**: Model Random Forest terbaik (`best_rf_classifier`), yang juga telah dioptimalkan, disimpan ke file `random_forest_model.joblib` menggunakan fungsi `joblib.dump`.
4. **Ekspor Dataset**: Dataset yang telah dibersihkan dan diproses (`df`) diekspor ke file CSV bernama `cleaned_student_data.csv` menggunakan `df.to_csv` dengan parameter `index=False` untuk menghilangkan indeks baris dalam file output.
5. **Output**: Pesan konfirmasi dicetak untuk menginformasikan bahwa model telah berhasil diekspor sebagai `decision_tree_model.joblib` dan `random_forest_model.joblib`, serta dataset telah diekspor sebagai `cleaned_student_data.csv`.

**Hasil**:  
- Model Decision Tree dan Random Forest berhasil disimpan sebagai file `joblib`, memungkinkan penggunaan kembali untuk prediksi pada aplikasi seperti dashboard Streamlit.
- Dataset yang telah dibersihkan disimpan sebagai `cleaned_student_data.csv`, yang dapat digunakan untuk analisis lebih lanjut atau sebagai input untuk model pada sistem lain.

## Ringkasan dan Temuan Utama

**Deskripsi**:  
Sel ini menyajikan ringkasan hasil analisis data dan pelatihan model untuk memprediksi dropout siswa di Jaya Jaya Institut. Berikut adalah poin-poin utama:

1. **Tujuan Utama**: Memprediksi siswa yang berpotensi dropout menggunakan model Decision Tree dan Random Forest untuk mengidentifikasi siswa berisiko guna memberikan bimbingan khusus.
2. **Model yang Digunakan**: Model Decision Tree dan Random Forest dievaluasi menggunakan metrik akurasi, presisi, recall, F1-score, AUC-ROC, dan matriks konfusi pada set pengujian.
3. **Performa Model**: Random Forest mengungguli Decision Tree pada semua metrik evaluasi, dengan akurasi ~0.77 dibandingkan ~0.70 untuk Decision Tree.
4. **Temuan Utama**:
   - Dataset tidak memiliki nilai hilang; outlier ditangani menggunakan metode IQR.
   - Fitur kategorikal diubah menjadi numerik dengan one-hot encoding (untuk kardinalitas rendah) dan label encoding (untuk kardinalitas tinggi).
   - Fitur baru dibuat, seperti `Interaction_Grade_Failed_Courses`, `Age_Squared`, `Interaction_Mother_Father_Qual`, `Academic_Performance_1st_Sem`, dan `Combined_Parents_Qual` untuk meningkatkan performa model.
   - Data dibagi menjadi 80% pelatihan, 10% validasi, dan 10% pengujian dengan stratifikasi untuk menjaga distribusi kelas.
   - GridSearchCV menghasilkan parameter optimal:
     - Decision Tree: `{'max_depth': 10, 'min_samples_leaf': 2, 'min_samples_split': 10}`.
     - Random Forest: `{'max_depth': None, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 200}`.
   - Ketidakseimbangan kelas terdeteksi, dengan performa lebih rendah pada kelas 1.0.
5. **Langkah Selanjutnya**:
   - Mengimplementasikan model pada dashboard Streamlit untuk prediksi dropout secara real-time.
"""